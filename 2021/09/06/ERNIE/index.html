<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="Lingfeng Zhu">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Lingfeng Zhu">
    
    <meta name="keywords" content="machine learning,deep learning,recommender system">
    
    <meta name="description" content="">
    <meta name="description" content="ERNIE: Enhanced Representation through Knowledge Integration 是百度在 2019 年 4 月提出的，基于 Google 的 BERT 模型优化的语义预训练模型。在诸多中文 NLP 任务上得到了 SOTA 的结果。">
<meta property="og:type" content="article">
<meta property="og:title" content="ERNIE">
<meta property="og:url" content="https://lingfengzhu.github.io/2021/09/06/ERNIE/index.html">
<meta property="og:site_name" content="Vinn&#39;s Studio">
<meta property="og:description" content="ERNIE: Enhanced Representation through Knowledge Integration 是百度在 2019 年 4 月提出的，基于 Google 的 BERT 模型优化的语义预训练模型。在诸多中文 NLP 任务上得到了 SOTA 的结果。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lingfengzhu.github.io/2021/09/06/ERNIE/1.png">
<meta property="og:image" content="https://lingfengzhu.github.io/2021/09/06/ERNIE/2.png">
<meta property="og:image" content="https://lingfengzhu.github.io/2021/09/06/ERNIE/3.png">
<meta property="article:published_time" content="2021-09-06T06:19:20.000Z">
<meta property="article:modified_time" content="2021-09-08T10:36:58.625Z">
<meta property="article:author" content="Lingfeng Zhu">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lingfengzhu.github.io/2021/09/06/ERNIE/1.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    
    <title>ERNIE · Vinn&#39;s Studio</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/assets/favicon.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >Vinn&#39;s Studio</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">ERNIE</a>
            </div>
    </div>
    
    <a class="home-link" href=/>Vinn's Studio</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(https://source.unsplash.com/random)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            ERNIE
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "NLP">NLP</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">3.2k</span>Reading time: <span class="post-count reading-time">12 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2021/09/06</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>ERNIE: Enhanced Representation through Knowledge Integration 是百度在 2019 年 4 月提出的，基于 Google 的 BERT 模型优化的语义预训练模型。在诸多中文 NLP 任务上得到了 SOTA 的结果。</p>
<a id="more"></a>
<p>近年来，随着深度学习的发展，模型参数规模飞速增长，相应地对数据集规模也提出了更高要求。对于大部分 NLP 任务来说，构建大规模的标注数据集成本过高，难度较大，因此，能够习得通用语言表示的<strong>基于大规模未标注语料库的预训练模型</strong>（Pretrained Models, PTM) 成为了一个不错的选择。对于句法和语义相关的任务，将预训练语义模型 Fine-tune 到下游任务，通常能够获得较好的结果。</p>
<h2 id="ERNIE-1-0"><a href="#ERNIE-1-0" class="headerlink" title="ERNIE-1.0"></a>ERNIE-1.0</h2><h3 id="Model-Abstract"><a href="#Model-Abstract" class="headerlink" title="Model Abstract"></a>Model Abstract</h3><p>ERNIE-1.0 对 BERT 的主要优化在于：</p>
<ol>
<li>对 BERT 的 mask 机制做了改进。ERNIE 的 mask 不是基本的 word piece 的 mask，而是在预训练阶段增加了外部的知识，由三种 level 的 mask 组成：basic-level masking（word piece），phrase-level masking（WWM style）以及 entity level masking。</li>
<li>在这个基础上，借助百度在中文的社区的强大能力，中文的 ERNIE 还使用了各种<strong>异质</strong>（Heterogeneous）的数据集进行预训练。</li>
<li>此外，为了适应<strong>多轮对话</strong>的贴吧数据，所有 ERNIE 均引入了 DLM (Dialogue Language Model) task。</li>
</ol>
<p>ERNIE 的优化思路简洁明了，是后来各种改进模型的基础。例如，ERNIE 的 masking 改进，反哺了 BERT，促成了中文 BERT 的 WWM 版本（Pre-Training with Whole Word Masking for Chinese BERT）以及 facebook 的 SpanBERT 等模型。百度的 ERNIE 1.0 只是针对中文进行的优化，且该模型内置于百度自家的 paddlepaddle 机器学习框架（而非业界主流的 TensorFlow 或者 pytorch），因此较少受到国外学者的关注。（参考 <a href="https://arxiv.org/abs/1904.09223" target="_blank" rel="noopener">ERNIE-1.0 论文地址</a> 以及 <a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" rel="noopener">ERNIE 项目地址</a>）</p>
<h3 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h3><h4 id="Pre-Training-Data-Heterogeneous-Corpus"><a href="#Pre-Training-Data-Heterogeneous-Corpus" class="headerlink" title="Pre-Training Data: Heterogeneous Corpus"></a>Pre-Training Data: Heterogeneous Corpus</h4><p>ERNIE-1.0 的预训练数据来自百度中文社区的各种<strong>异质</strong>数据集（Heterogeneous Corpus），包括：</p>
<ul>
<li>Chinese Wikepedia：中文维基百科，包括的 sentences 数目为 21M；</li>
<li>Baidu Baike：百度百科，包括的 sentences 数目为 51M；</li>
<li>Baidu news：百度新闻，包括的 sentences 数目为 47M ；</li>
<li>Baidu Tieba：百度贴吧，包括的 sentences 数目为 54M；</li>
</ul>
<p>基于上述<strong>百科类、资讯类、论坛对话类</strong>等数据集构造一批具有上下文关系的<strong>句子对</strong>数据作为预训练数据。此处对文本数据进行了繁简体的转化，且数据不区分大小写（uncased）。</p>
<blockquote>
<ul>
<li>BERT 的英文语料，包括 800M words 的 BooksCorpus，和包含 2,500M words 的英文 Wikipedia；</li>
<li>ERNIE中文语料共 173M sentences。如果按照每个 sentence 平均 30 words 估计，可以达到5000M words 量级，还是相当充足的。同时，ERNIE 的语料类型非常丰富：百科包含大量实体和短语信息，且内容质量很高；百度新闻则有利于模型学习新实体和新短语；百度贴吧则有利于模型学习口语表达，提升泛化能力。</li>
</ul>
</blockquote>
<h4 id="Pre-Training-Input"><a href="#Pre-Training-Input" class="headerlink" title="Pre-Training Input"></a>Pre-Training Input</h4><p>预训练阶段，基于上述<strong>句子对</strong>数据，使用如下步骤进行预处理：（相关代码参见 <a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" rel="noopener">ERNIE 项目地址</a>）</p>
<ol>
<li>利用百度内部词法分析工具对句对数据进行<strong>字、词、实体</strong>等不同粒度的切分，然后基于 <code>tokenization.py</code> 中的 <code>CharTokenizer</code> 对切分后的数据进行 token 化处理（中文分词处理，将一句话切分为多个 token，每个 token 代表一个词语），得到明文的 token 序列（序列的每个位置代表一个字）及对应的切分边界（指示出 token 序列中不同 token 的切分边界位置）；</li>
<li>然后将明文数据根据词典 <code>config/vocab.txt</code> 映射为 id 数据；</li>
<li>根据切分边界对连续的 tokens 进行一些随机 mask 操作，预训练时，将 mask 掉的部分作为 label，其他部分作为输入的 feature 构建预训练任务。</li>
</ol>
<p>经过上述预处理之后的一个输入样例（一个句子对）包含如下几个部分：</p>
<ul>
<li><code>token_ids</code>：输入<strong>句子对</strong>中的每个字的 id 表示；</li>
<li><code>sentence_type_ids</code>：0 或者 1，表示对应的字属于句子对中的哪一个句子；</li>
<li><code>position_ids</code>：绝对位置编码，标记出对应的字在输入序列中的位置；</li>
<li><code>seg_labels</code>：表示分词边界信息（指示出不同词的切分位置），0表示词首、1表示非词首、-1为占位符；</li>
<li><code>next_sentence_label</code>：表示该<strong>句子对</strong>是否存在<strong>前后句</strong>关系（0 为无 1 为有）</li>
</ul>
<blockquote>
<p>一个输入样例（一个句子对）的实例：</p>
<p>11048 492 1333 1361 1051 326 2508 5 1803 1827 98 164 133 2777 2696 983 121 4 199 634 551 844 85 14 2476 1895 33 13 983 121 23 7 1093 24 46 660 12043 2 1263 6328 33 121 126 398 276 315 5 63 44 35 25 12043 2;</p>
<p>0 0 0 0 0 0 0 0 0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1;</p>
<p>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55;</p>
<p>-1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 00 -1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 -1;</p>
<p>0</p>
</blockquote>
<h4 id="Fine-tune-Input"><a href="#Fine-tune-Input" class="headerlink" title="Fine-tune Input"></a>Fine-tune Input</h4><p>在使用预训练模型进行<strong>微调</strong>的阶段，对于<strong>下游任务</strong>（即要处理的实际问题）的数据集，使用类似 Transformer 的输入形式进行预处理并输入模型：（详情参见 Transformer）</p>
<ol>
<li><p>获取输入句子的每一个单词对应的表示向量：</p>
<ul>
<li>词向量由单词的 Embedding （可以采用 <code>Word2Vec</code>、<code>GloVe</code> 等算法预训练得到，或直接在 ERNIE 中训练得到）和单词位置的 Embedding 相加得到</li>
<li>词向量维度一般可以设定为 512</li>
</ul>
</li>
<li><p>将得到的词组合成向量矩阵</p>
<ul>
<li>矩阵的每一行就是一个词向量</li>
<li>矩阵的行数就是句子中单词的个数</li>
</ul>
</li>
</ol>
<h4 id="Special-Tokens"><a href="#Special-Tokens" class="headerlink" title="Special Tokens"></a>Special Tokens</h4><p>ERNIE-1.0 中包含的<strong>特殊 token</strong> 基本上类似 BERT，用于标记一些特殊的信息，在模型中的地位完全等同于一个普通 token。包含如下几类：</p>
<ul>
<li><code>[CLS]</code>：针对不同类型的实际任务，BERT 会在每条样本的开头加上一个对应该任务的符号。例如，对于分类任务，BERT 会在输入词语序列前加上一个特殊的符号 <code>[CLS]</code> 表示分类任务；</li>
<li><code>[SEP]</code>：插入在不同的句子中间，用于区分不同的句子；</li>
<li><code>[PAD]</code>：占位符，预训练 model 的接口 api 只能接受长度相同的 input，所以用 <code>[pad]</code> 让所有样本的长度都能够对齐。若输入样本太短，则在样本后补上一定数量的 <code>[PAD]</code> 直到样本长度达到预设的值；若输入样本太长，则可以直接做截断，去掉超出预设长度的部分；</li>
<li><code>[UNK]</code>：表示未知字符。设置该字符更多是为预测阶段服务，如果 input 的样本里存在未知的词（word embedding 矩阵/词表里没有的 token），则在 token 化分词的时候，这类词都会变成 <code>[UNK]</code>。BERT 的 embedding 矩阵里有一行 embedding 词向量专门用来表示<code>[UNK]</code>，不过并未公布该向量具体是如何设计的。最简单的方法就是直接取所有 embedding vector 的均值作为 <code>[UNK]</code> 的词嵌入向量；</li>
<li><code>[MASK]</code>：掩码，用于标记被 mask 掉的 token。它的存在主要是为了 MLM (Masked Language Model) 任务，BERT 的 embedding 矩阵中也有专门存放 <code>[MASK]</code> 这个 token 的 embedding 向量，设计的目的也是为了保证程序正常运行，如果 embedding layer 里没有 <code>[MASK]</code> 则程序报错。</li>
</ul>
<h4 id="Construction"><a href="#Construction" class="headerlink" title="Construction"></a>Construction</h4><p>ERNIE-1.0 基本上是对 transformer 的 encoder 部分的改进，并且每个 encoder block 在结构上完全一致，只是 block 之间不共享权重。具体区别如下:</p>
<ul>
<li>Transformer: 6 encoder layers, 512 hidden units, 8 attention heads</li>
<li>ERNIE Base: 12 encoder layers, 768 hidden units, 12 attention heads</li>
<li>ERNIE Large: 24 encoder layers, 1024 hidden units, 16 attention heads</li>
</ul>
<p>除了上述区别外，ERNIE-1.0 其他部分的结构与 Transformer/BERT 一致。</p>
<h4 id="Pre-Training-Strategy"><a href="#Pre-Training-Strategy" class="headerlink" title="Pre-Training Strategy"></a>Pre-Training Strategy</h4><p>除了贴吧<strong>多轮对话</strong>数据外，ERNIE 采用的是应该都是普通的 NSP (Next Sentence Prediction) + MLM (Masked Language Model) 预训练任务。</p>
<h4 id="Knowledge-Masking"><a href="#Knowledge-Masking" class="headerlink" title="Knowledge Masking"></a>Knowledge Masking</h4><p>BERT 在预训练的过程中，会将语句的随机汉字或单词进行 mask，将这些被 mask 掉的部分作为 label 构建预训练任务让模型去学习。这样的 masking 方式存在缺陷：</p>
<blockquote>
<p> 例如，输入文本“哈利波特是 J.K.罗琳写的小说”。如果使用 <code>哈[MASK]波特</code> 或者 <code>J.K.[MASK]琳</code> 这样的  mask 策略，模型可以轻松地预测出 <code>利</code> 和 <code>罗</code>，但是模型不能学习到 <code>哈利波特</code> 和 <code>J.K. 罗琳</code> 间的关系；而如果把 <code>哈利波特</code> 作为一个实体直接 mask 掉的话，那模型就可以做到根据作者预测到小说这个实体，实现了知识的学习。</p>
</blockquote>
<img src="/2021/09/06/ERNIE/1.png" class="" title="This is an image">
<p>相比于BERT，ERNIE-1.0 改进新增了两种 masking 策略：一种是基于 phrase（短语，比如 a series of, written等）的 masking 策略，将由多个汉字或单词组成的 phrase 作为一个统一单元进行 mask；另外一种是基于 entity（实体，在这里是人名，位置，组织，产品等名词，比如 Apple, J.K. Rowling）的 masking 策略，将由多个汉字或单词组成的 entity 作为一个统一单元进行 mask。</p>
<p> 对比直接将知识类的 query 映射成向量然后直接加起来，ERNIE 通过统一 mask 的方式可以潜在的学习到知识的依赖以及更长的语义依赖来让模型更具泛化性。需要注意的是这些知识的学习是在训练中隐性地学习，而不是直接将外部知识的 embedding 加入到模型结构中（这是 ERNIE-TsingHua 的做法），模型在训练中学习到了更长的语义联系，例如说实体类别，实体关系等，这些都使得模型可以学习到更好的语言表达。</p>
<ul>
<li><p>Basic level masking：在预训练中，第一阶段先采用基本层级的 masking，对于每一个字，以随机概率进行 mask；</p>
</li>
<li><p>Phrase level masking：第二阶段是采用词组级别的 masking，通过分词工具来获得中文短语/通过词汇分析来判断英文短语，然后随机 mask 掉句子中一部分短语词组，然后让模型预测这些词组，在这个阶段，词组的信息就被 encoding 到 word embedding 中了；</p>
</li>
<li>Entity level masking：第三阶段， 首先识别出命名实体（例如人名，机构名，商品名等），然后以一定概率随机 mask 掉这些实体，模型在训练完成后，也就学习到了这些实体的信息。</li>
</ul>
<img src="/2021/09/06/ERNIE/2.png" class="" title="This is an image">
<blockquote>
<p>通过阅读源码，其实可以发现 multi-stage knowledge masking strategy 的逻辑是：在每个 epoch，以某个概率（该数据未开源）随机选择 <code>mask_word</code> 或者 <code>mask_char</code> 模式：<code>mask_word</code> 模式后续会走 phrase-level masking 逻辑，会利用到分词边界信息；mask_char 模式后续会走 basic-level masking 的逻辑，不会利用分词边界信息。</p>
<p>Phrase-level masking 的实现：</p>
<ol>
<li>与 BERT 相对的，ERNIE 这一步是在每个 epoch 动态生成的，且 masking 的数目没有上限（没有 <code>max_predictions_per_seq</code> 的限制）；</li>
<li>基于分词信息，以词为单位决定是否触发 masking 逻辑；</li>
<li>对于每一个需要 mask 的字符串（一个词），遍历其中每个输入（一个字符）：<ul>
<li>80% 的概率，把输入替换为 [MASK]；</li>
<li>10% 的概率，把输入替换为随机的 token（随机字符）；</li>
<li>10% 的概率，维持原输入不变；</li>
</ul>
</li>
</ol>
</blockquote>
<h4 id="DLM-Dialogue-Language-Model-task"><a href="#DLM-Dialogue-Language-Model-task" class="headerlink" title="DLM (Dialogue Language Model) task"></a>DLM (Dialogue Language Model) task</h4><p><strong>对话数据</strong>对语义表示很重要，因为对于相同回答的提问一般都是具有类似语义的。ERNIE 修改了 BERT 的输入形式，使得样本能够以<strong>多轮对话</strong>的形式输入模型：</p>
<ul>
<li>采用的三个句子 <code>S1</code>、<code>S2</code> 和 <code>S3</code> 的组合作为一个输入样本：<code>[CLS]S1[SEP]S2[SEP]S3[SEP]</code> 的格式；</li>
<li>对于每个输入样本，新增 dialog embedding 来标记<strong>多轮对话</strong>信息（类似 segment embedding）：例如，使用 Q 表示<strong>提问</strong>，R 表示<strong>回答</strong>，则三个句子组成的输入样本可能有 QRQ，QRR，QQR 等不同的形式；</li>
<li>DLM 还增加了任务来判断这个多轮对话是真的还是假的；</li>
</ul>
<img src="/2021/09/06/ERNIE/3.png" class="" title="This is an image">
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://cloud.tencent.com/developer/article/1687288" target="_blank" rel="noopener">详解ERNIE-Baidu进化史及应用场景</a></p>
<p><a href="https://ai.baidu.com/forum/topic/show/954092" target="_blank" rel="noopener">超详细中文预训练模型ERNIE使用指南</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/88118475" target="_blank" rel="noopener">一文读懂最强中文NLP预训练模型ERNIE</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/376861524" target="_blank" rel="noopener">5分钟理解百度ERNIE核心思想</a></p>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>Author：<a href="https://lingfengzhu.github.io">Lingfeng Zhu</a>
            <p>Original Link：<a href="https://lingfengzhu.github.io/2021/09/06/ERNIE/">https://lingfengzhu.github.io/2021/09/06/ERNIE/</a> 
            <p>Published：<a href="https://lingfengzhu.github.io/2021/09/06/ERNIE/">September 6th 2021, 2:19:20 pm</a>
            <p>Updated：<a href="https://lingfengzhu.github.io/2021/09/06/ERNIE/">September 8th 2021, 6:36:58 pm</a>
            <p>Copyright：This Article Uses <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">Attribution-NonCommercial 4.0 International</a> as the license</p> 
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2021/09/03/Two-Pointer/" title= "Two-Pointer">
                    <div class="prevTitle">Two-Pointer</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- gitalk评论 -->

    <!-- utteranc评论 -->

    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:jolin.windy072@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/LingfengZhu" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                  
                  <img class="profile-qr" src="/assets/wechatQR.jpeg" />
                </span>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <a href="//www.facebook.com/lingfeng.zhu.18/" class="iconfont-archer facebook" target="_blank" title=facebook></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <a href="//www.linkedin.com/in/lingfeng-zhu-256315193/" class="iconfont-archer linkedin" target="_blank" title=linkedin></a>
            
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="about">Powered by <a href="https://lingfengzhu.github.io/about/" target="_blank">Vinn</a></span><span class="iconfont-archer power">&#xe635;</span><span id="instituion-info">from <a href="https://www.wisc.edu/" target="_blank">UWM</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#ERNIE-1-0"><span class="toc-number">1.</span> <span class="toc-text">ERNIE-1.0</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Model Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Construction"><span class="toc-number">1.2.</span> <span class="toc-text">Model Construction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pre-Training-Data-Heterogeneous-Corpus"><span class="toc-number">1.2.1.</span> <span class="toc-text">Pre-Training Data: Heterogeneous Corpus</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pre-Training-Input"><span class="toc-number">1.2.2.</span> <span class="toc-text">Pre-Training Input</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Fine-tune-Input"><span class="toc-number">1.2.3.</span> <span class="toc-text">Fine-tune Input</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Special-Tokens"><span class="toc-number">1.2.4.</span> <span class="toc-text">Special Tokens</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Construction"><span class="toc-number">1.2.5.</span> <span class="toc-text">Construction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pre-Training-Strategy"><span class="toc-number">1.2.6.</span> <span class="toc-text">Pre-Training Strategy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-Masking"><span class="toc-number">1.2.7.</span> <span class="toc-text">Knowledge Masking</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DLM-Dialogue-Language-Model-task"><span class="toc-number">1.2.8.</span> <span class="toc-text">DLM (Dialogue Language Model) task</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">2.</span> <span class="toc-text">参考资料</span></a></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 20
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2021 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/06</span><a class="archive-post-title" href= "/2021/09/06/ERNIE/" >ERNIE</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/03</span><a class="archive-post-title" href= "/2021/09/03/Two-Pointer/" >Two-Pointer</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span><a class="archive-post-title" href= "/2021/08/30/Federated-Learning/" >Federated Learning</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/27</span><a class="archive-post-title" href= "/2021/08/27/Transfer-Learning/" >Transfer Learning</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/29</span><a class="archive-post-title" href= "/2021/07/29/Decorator/" >Python Decorator</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/29</span><a class="archive-post-title" href= "/2021/07/29/Python-args-and-kwargs/" >Python Syntax: *args and **kwargs</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/05</span><a class="archive-post-title" href= "/2021/07/05/Python-Logging/" >Python Logging</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/25</span><a class="archive-post-title" href= "/2021/05/25/BERT/" >BERT</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/24</span><a class="archive-post-title" href= "/2021/05/24/Conda/" >Conda</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/05</span><a class="archive-post-title" href= "/2021/02/05/Docker/" >Docker</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/06</span><a class="archive-post-title" href= "/2021/01/06/Trie-Tree/" >Trie Tree</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2020 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/08</span><a class="archive-post-title" href= "/2020/12/08/Gradient-Descent/" >Gradient Descent</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span><a class="archive-post-title" href= "/2020/11/16/Time-Complexity/" >Time Complexity</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/15</span><a class="archive-post-title" href= "/2020/11/15/SortAlgorithm/" >Sort Algorithm</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span><a class="archive-post-title" href= "/2020/06/29/BinarySearch/" >Binary Search</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/26</span><a class="archive-post-title" href= "/2020/04/26/Delta-Method-in-Deep-Learning/" >Delta Method in Deep Learning</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/18</span><a class="archive-post-title" href= "/2020/04/18/BackTrack/" >Backtracking</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/16</span><a class="archive-post-title" href= "/2020/04/16/Python-Syntax-with/" >Python Syntax: with</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/27</span><a class="archive-post-title" href= "/2020/03/27/COVID-Survival-Anaysis/" >COVID-19 Survival Analysis</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/27</span><a class="archive-post-title" href= "/2020/03/27/Hexo-Guideline/" >Hexo Guideline</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="NLP"><span class="iconfont-archer">&#xe606;</span>NLP</span>
    
        <span class="sidebar-tag-name" data-tags="Conda"><span class="iconfont-archer">&#xe606;</span>Conda</span>
    
        <span class="sidebar-tag-name" data-tags="Survival Analysis"><span class="iconfont-archer">&#xe606;</span>Survival Analysis</span>
    
        <span class="sidebar-tag-name" data-tags="COVID-19"><span class="iconfont-archer">&#xe606;</span>COVID-19</span>
    
        <span class="sidebar-tag-name" data-tags="Python"><span class="iconfont-archer">&#xe606;</span>Python</span>
    
        <span class="sidebar-tag-name" data-tags="Docker"><span class="iconfont-archer">&#xe606;</span>Docker</span>
    
        <span class="sidebar-tag-name" data-tags="Hexo"><span class="iconfont-archer">&#xe606;</span>Hexo</span>
    
        <span class="sidebar-tag-name" data-tags="Web-Frontend"><span class="iconfont-archer">&#xe606;</span>Web-Frontend</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="Deep-Learning"><span class="iconfont-archer">&#xe60a;</span>Deep-Learning</span>
    
        <span class="sidebar-category-name" data-categories="Algorithm"><span class="iconfont-archer">&#xe60a;</span>Algorithm</span>
    
        <span class="sidebar-category-name" data-categories="Syntax"><span class="iconfont-archer">&#xe60a;</span>Syntax</span>
    
        <span class="sidebar-category-name" data-categories="Statistics"><span class="iconfont-archer">&#xe60a;</span>Statistics</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Lingfeng Zhu"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->    
     
    </body>
</html>


